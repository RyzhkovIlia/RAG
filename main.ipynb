{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wait for Milvus Starting...\n",
      "Start successfully.\n",
      "To change the default Milvus configuration, add your settings to the user.yaml file and then restart the service.\n"
     ]
    }
   ],
   "source": [
    "# Lets launch Milvus (you need to run Docker)\n",
    "!bash standalone_embed.sh start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the necessary tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/RAG/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pickle\n",
    "from typing import List\n",
    "from numpy import ndarray\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import google.generativeai as genai\n",
    "from langchain.text_splitter import SentenceTransformersTokenTextSplitter\n",
    "from langchain.embeddings.base import Embeddings\n",
    "from langchain_milvus.retrievers import MilvusCollectionHybridSearchRetriever\n",
    "from langchain_milvus.utils.sparse import BM25SparseEmbedding\n",
    "from pymilvus import (\n",
    "    Collection,\n",
    "    CollectionSchema,\n",
    "    DataType,\n",
    "    FieldSchema,\n",
    "    WeightedRanker,\n",
    "    connections,\n",
    "    utility\n",
    ")\n",
    "from pdfminer.high_level import extract_text\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# connect to Milvus\n",
    "CONNECTION_URI = \"http://localhost:19530\"\n",
    "connections.connect(uri=CONNECTION_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdf_file(file_path: str, start_page: int, end_page: int) -> list:\n",
    "    \"\"\"\n",
    "    Extracts and processes text from a specified range of pages in a PDF file.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path (str): The path to the PDF file to be processed.\n",
    "    - start_page (int): The page number where text extraction should begin.\n",
    "    - end_page (int): The page number where text extraction should end.\n",
    "\n",
    "    Returns:\n",
    "    - list: A list of text segments (split by two newlines) extracted from the specified pages of the PDF. \n",
    "            The text is cleaned by removing content within parentheses and other formatting artifacts.\n",
    "    \"\"\"\n",
    "    text = extract_text(file_path, page_numbers=list(range(start_page, end_page)))\n",
    "    text = re.sub(r'\\(.*?\\)', '', text.replace('\\x0c', ''), flags=re.DOTALL)\n",
    "    text = text.split('\\n\\n')\n",
    "\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def split_text_to_token(text_split: list) -> List[str]:\n",
    "    \"\"\"\n",
    "    Splits a list of text segments into smaller chunks based on sentence boundaries and token limits.\n",
    "\n",
    "    Parameters:\n",
    "    - text_split (list): A list of text segments to be split into smaller chunks.\n",
    "\n",
    "    Returns:\n",
    "    - List[str]: A list of smaller text chunks, each containing no more than the defined token limit \n",
    "                 (256 tokens per chunk) with no overlapping tokens between chunks.\n",
    "    \"\"\"\n",
    "    token_splitter = SentenceTransformersTokenTextSplitter(chunk_overlap=0, tokens_per_chunk=256)\n",
    "    token_split_texts = []\n",
    "    for text in [i for i in text_split if i != \"\"]:\n",
    "        token_split_texts += token_splitter.split_text(text)\n",
    "\n",
    "    return token_split_texts\n",
    "\n",
    "class SentenceTransformerEmbeddings(Embeddings):\n",
    "    \"\"\"\n",
    "    A class for generating dense embeddings for documents and queries using a SentenceTransformer model.\n",
    "    \n",
    "    This class provides methods to embed documents and queries, making it suitable for use in information \n",
    "    retrieval tasks. It also includes functionality to save the embedding model to a specified path.\n",
    "\n",
    "    Args:\n",
    "        model_name (str, optional): The name of the SentenceTransformer model to use for embeddings.\n",
    "            Defaults to \"all-MiniLM-L6-v2\".\n",
    "\n",
    "    Attributes:\n",
    "        model (SentenceTransformer): The SentenceTransformer model used for encoding documents and queries.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name:str=\"all-MiniLM-L6-v2\"):\n",
    "        \"\"\"\n",
    "        Initializes the SentenceTransformerEmbeddings instance with a specified SentenceTransformer model.\n",
    "        \n",
    "        Args:\n",
    "            model_name (str, optional): The name of the SentenceTransformer model to use. \n",
    "                Defaults to \"all-MiniLM-L6-v2\".\n",
    "        \"\"\"\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "\n",
    "    def embed_documents(self, documents:list) -> list:\n",
    "        \"\"\"\n",
    "        Generates embeddings for a list of documents.\n",
    "\n",
    "        Args:\n",
    "            documents (list of str): A list of documents to be embedded.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: An array of embeddings for the provided documents.\n",
    "        \"\"\"\n",
    "        return self.model.encode(documents)\n",
    "\n",
    "    def embed_query(self, query:str) -> ndarray:\n",
    "        \"\"\"\n",
    "        Generates an embedding for a single query.\n",
    "\n",
    "        Args:\n",
    "            query (str): A query to be embedded.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: An embedding for the provided query.\n",
    "        \"\"\"\n",
    "        return self.model.encode([query])[0]\n",
    "    \n",
    "    def save(self, path:str) -> None:\n",
    "        \"\"\"\n",
    "        Saves the SentenceTransformer model to the specified path.\n",
    "\n",
    "        Args:\n",
    "            path (str): The path where the model should be saved.\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        self.model.save(path)\n",
    "    \n",
    "def save_params_for_retriever(\n",
    "    collection: Collection, \n",
    "    dense_field: str,\n",
    "    sparse_field: str,\n",
    "    dense_embedding_func: SentenceTransformerEmbeddings,\n",
    "    sparse_embedding_func: BM25SparseEmbedding\n",
    "    ) -> None:\n",
    "    \"\"\"\n",
    "    Saves the parameters for setting up a Milvus hybrid retriever to a configuration file.\n",
    "\n",
    "    Args:\n",
    "        collection (Collection): The Milvus collection for which retriever parameters are being saved.\n",
    "        dense_field (str): The name of the field for dense embeddings within the collection.\n",
    "        sparse_field (str): The name of the field for sparse embeddings within the collection.\n",
    "        dense_embedding_func (SentenceTransformerEmbeddings): The embedding function for dense text representations.\n",
    "        sparse_embedding_func (BM25SparseEmbedding): The embedding function for sparse text representations.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    dense_embedding_func.save(f'src/embed_models/dense_embedding_model_{collection.name}')\n",
    "    config = {\n",
    "        'colection_name': collection.name,\n",
    "        'dense_field': dense_field,\n",
    "        'sparse_field': sparse_field,\n",
    "        'sparse_embedding_func': sparse_embedding_func\n",
    "    }\n",
    "\n",
    "    with open(f\"src/configs/config_params_{collection.name}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(config, f)\n",
    "\n",
    "\n",
    "def get_hybrid_function(\n",
    "        text_field: str, \n",
    "        name_of_collection: str,\n",
    "        k: int=5, \n",
    "        metric: str='IP'\n",
    "    ) -> MilvusCollectionHybridSearchRetriever:\n",
    "    \"\"\"\n",
    "    Creates and returns a MilvusCollectionHybridSearchRetriever instance, configured for hybrid search\n",
    "    with both dense and sparse embeddings.\n",
    "\n",
    "    Args:\n",
    "        text_field (str): The name of the field containing text data in the Milvus collection.\n",
    "        config_path (str): The path to the configuration file (in pickle format) containing collection \n",
    "            and embedding settings.\n",
    "        k (int, optional): The number of top results to retrieve. Defaults to 5.\n",
    "        metric (str, optional): The similarity metric to use for search (e.g., 'IP' for inner product). Defaults to 'IP'.\n",
    "\n",
    "    Returns:\n",
    "        MilvusCollectionHybridSearchRetriever: A configured hybrid search retriever for searching\n",
    "            within the Milvus collection using both dense and sparse embeddings.\n",
    "    \"\"\"\n",
    "    with open(f'src/configs/config_params_{name_of_collection}.pkl', \"rb\") as f:\n",
    "        config = pickle.load(f)\n",
    "    model = SentenceTransformerEmbeddings(f'src/embed_models/dense_embedding_model_{name_of_collection}')\n",
    "    sparse_search_params = {\"metric_type\": metric}\n",
    "    dense_search_params = {\"metric_type\": metric, \"params\": {}}\n",
    "    retriever = MilvusCollectionHybridSearchRetriever(\n",
    "        collection = Collection(name_of_collection),\n",
    "        rerank=WeightedRanker(0.5, 0.5),\n",
    "        anns_fields=[config['dense_field'], config['sparse_field']],\n",
    "        field_embeddings=[model, config['sparse_embedding_func']],\n",
    "        field_search_params=[dense_search_params, sparse_search_params],\n",
    "        top_k=k,\n",
    "        text_field=text_field,\n",
    "        nprobe=20\n",
    "    )\n",
    "\n",
    "    return retriever\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create MILVUS Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection Tresis was remove.\n",
      "All of collections were remove\n"
     ]
    }
   ],
   "source": [
    "# If you need, you can delete  previous collections\n",
    "\n",
    "collections = utility.list_collections()\n",
    "\n",
    "# Remove each collection from the list\n",
    "for collection_name in collections:\n",
    "    collection = Collection(name=collection_name)\n",
    "    collection.drop()\n",
    "    print(f\"Collection {collection_name} was remove.\")\n",
    "\n",
    "print(\"All of collections were remove\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "pk_field = \"doc_id\"\n",
    "dense_field = \"dense_vector\"\n",
    "sparse_field = \"sparse_vector\"\n",
    "text_field = \"text\"\n",
    "fields = [\n",
    "    FieldSchema(\n",
    "        name=pk_field,\n",
    "        dtype=DataType.VARCHAR,\n",
    "        is_primary=True,\n",
    "        auto_id=True,\n",
    "        max_length=100,\n",
    "    ),\n",
    "    FieldSchema(name=dense_field, dtype=DataType.FLOAT_VECTOR, dim=384),\n",
    "    FieldSchema(name=sparse_field, dtype=DataType.SPARSE_FLOAT_VECTOR),\n",
    "    FieldSchema(name=text_field, dtype=DataType.VARCHAR, max_length=65_535),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the collection\n",
    "schema = CollectionSchema(fields=fields, enable_dynamic_field=False)\n",
    "collection = Collection(\n",
    "    name=\"Tresis\", schema=schema, consistency_level=\"Strong\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add two\n",
    "dense_index = {\"index_type\": \"FLAT\", \"metric_type\": \"IP\"}\n",
    "collection.create_index(\"dense_vector\", dense_index)\n",
    "sparse_index = {\"index_type\": \"SPARSE_INVERTED_INDEX\", \"metric_type\": \"IP\"}\n",
    "collection.create_index(\"sparse_vector\", sparse_index)\n",
    "collection.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load split text from our pdf file\n",
    "pdf_text = load_pdf_file(file_path='dataset/example.pdf', start_page=13, end_page=158)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing text (you need to make own preprocessing for own pdf)\n",
    "pdf_text = [p.replace('\\n', ' ').strip() for p in pdf_text if (not re.search(r'© Springer International Publishing Switzerland 2014', p.replace('\\n', ' ').strip())) and (not p.replace('\\n', ' ').strip().isdigit())]\n",
    "pdf_text_2 = []\n",
    "for i in pdf_text:\n",
    "    if i.split(' ')[0].istitle() or i.split(' ')[0] == '•':\n",
    "        pdf_text_2.append(i)\n",
    "    else:\n",
    "        pdf_text_2[-1]+=i\n",
    "\n",
    "chunked_text = split_text_to_token(text_split=pdf_text_2)\n",
    "chunked_text_2 = []\n",
    "for i in chunked_text:\n",
    "    if not len(i.strip())<5:\n",
    "        chunked_text_2.append(i.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dense and sparse embedding functions\n",
    "dense_embedding_func = SentenceTransformerEmbeddings()\n",
    "sparse_embedding_func = BM25SparseEmbedding(corpus=chunked_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert our text to Milvus Database\n",
    "entities = []\n",
    "for text in chunked_text_2:\n",
    "    entity = {\n",
    "        dense_field: dense_embedding_func.embed_documents([text])[0],\n",
    "        sparse_field: sparse_embedding_func.embed_documents([text])[0],\n",
    "        text_field: text,\n",
    "    }\n",
    "    entities.append(entity)\n",
    "collection.insert(entities)\n",
    "collection.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all database params for predict\n",
    "save_params_for_retriever(\n",
    "    collection=collection, \n",
    "    dense_field=dense_field, \n",
    "    sparse_field=sparse_field,\n",
    "    dense_embedding_func=dense_embedding_func,\n",
    "    sparse_embedding_func=sparse_embedding_func\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict our RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_of_collection = 'Tresis'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get hybrid searh function\n",
    "retriever = get_hybrid_function(\n",
    "    text_field='text', \n",
    "    name_of_collection=name_of_collection\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_rag_prompt(\n",
    "        query:str, \n",
    "        relevant_passage:str\n",
    "    ) -> str:\n",
    "    \"\"\"\n",
    "    Creates a prompt for a Retrieval-Augmented Generation (RAG) model, designed to answer questions using relevant text.\n",
    "\n",
    "    Parameters:\n",
    "    - query (str): The question that the assistant needs to answer.\n",
    "    - relevant_passage (str): The text passage that contains relevant information for answering the query.\n",
    "\n",
    "    Returns:\n",
    "    - str: A formatted prompt for the assistant, including the query and relevant context, structured for a friendly, non-technical response in Russian.\n",
    "    \"\"\"\n",
    "    escaped = relevant_passage.replace(\"'\", \"\").replace('\"', \"\").replace(\"\\n\", \" \")\n",
    "    prompt = f\"\"\"\n",
    "    Human: You are a helpful and informative bot that answers questions using text from the passage below. \n",
    "    Be sure to answer in a full sentence, exhaustively, including all relevant background information.\n",
    "    However, you are speaking to a non-technical audience, so be sure to break down complex concepts and\n",
    "    keep your tone friendly and accommodating.\n",
    "    If the passage is not relevant to the answer, you can ignore it.\n",
    "    Context is given between <context>.\n",
    "    The question is given between </question>.\n",
    "    Thank you!\n",
    "\n",
    "    <context>\n",
    "    {escaped}\n",
    "    </context>\n",
    "\n",
    "    <question>\n",
    "    {query}\n",
    "    </question>\n",
    "\n",
    "    Assistant:\n",
    "    \"\"\"\n",
    "\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gpt_model() -> genai.GenerativeModel:\n",
    "    \"\"\"\n",
    "    Initializes and returns a Generative AI model from the genai library using the Gemini API.\n",
    "\n",
    "    This function retrieves the Gemini API key from the environment variables, configures \n",
    "    the genai library, and initializes a generative model. If the API key is not provided, \n",
    "    it raises an error.\n",
    "\n",
    "    Returns:\n",
    "        genai.GenerativeModel: An instance of the generative model configured with the 'gemini-pro' model name.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the Gemini API key is not set in the environment variables as GEMINI_API_KEY.\n",
    "    \"\"\"\n",
    "    load_dotenv()\n",
    "    gemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "    if not gemini_api_key:\n",
    "        raise ValueError(\"Gemini API Key not provided. Please provide GEMINI_API_KEY as an environment variable\")\n",
    "    genai.configure(api_key=gemini_api_key)\n",
    "    model = genai.GenerativeModel('gemini-pro')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer_promt(\n",
    "        query:str, \n",
    "        retriever:MilvusCollectionHybridSearchRetriever, \n",
    "        model:genai.GenerativeModel\n",
    "    ) -> str:    \n",
    "    \"\"\"\n",
    "    Generates an answer to a query by using a retrieval-based prompt and a generative model.\n",
    "\n",
    "    Parameters:\n",
    "    - query (str): The question that the assistant should answer.\n",
    "    - retriever (MilvusCollectionHybridSearchRetriever): The retriever object used to fetch relevant documents for the query.\n",
    "    - model (genai.GenerativeModel): The generative model used to generate the answer based on the prompt.\n",
    "\n",
    "    Returns:\n",
    "    - str: The model's generated answer in response to the query, based on the provided relevant context.\n",
    "    \"\"\"\n",
    "    similar_docs = retriever.invoke(query)\n",
    "    relevant_text = \". \".join([doc.page_content.capitalize() for doc in similar_docs])\n",
    "    prompt = make_rag_prompt(query=query, relevant_passage=relevant_text)\n",
    "    answer = model.generate_content(prompt)\n",
    "    \n",
    "    return answer.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Examiners assess doctoral theses based on eight specific criteria, ensuring that the candidate possesses the necessary knowledge and skills to join their academic community. Feedback from examiners is then summarized in an appendix, providing valuable insights for candidates to enhance the quality of their work. By carefully reviewing these examiner reports and understanding the expectations, candidates can effectively prepare for their thesis examination and demonstrate their readiness for research and scholarship.'"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get predict from our questoin\n",
    "model = get_gpt_model()\n",
    "query=\"What factors do examiners consider when evaluating a thesis, and how do they use examiner reports to improve the quality of the candidate’s work?\"\n",
    "answer = generate_answer_promt(query=query, model=model, retriever=retriever)\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collection.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings = db.get(include=['embeddings'])['embeddings']\n",
    "# umap_transform = umap.UMAP(random_state=0, transform_seed=0).fit(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def project_embeddings(embeddings, umap_transform):\n",
    "#     umap_embeddings = np.empty((len(embeddings),2))\n",
    "#     for i, embedding in enumerate(tqdm(embeddings)): \n",
    "#         umap_embeddings[i] = umap_transform.transform([embedding])\n",
    "#     return umap_embeddings   \n",
    "\n",
    "# projected_dataset_embeddings = project_embeddings(embeddings, umap_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_embedding = embedding_function([query])[0]\n",
    "# retrieved_embeddings = relevant_text['embeddings'][0]\n",
    "\n",
    "# projected_query_embedding = project_embeddings([query_embedding], umap_transform)\n",
    "# projected_retrieved_embeddings = project_embeddings(retrieved_embeddings, umap_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure()\n",
    "# plt.scatter(projected_dataset_embeddings[:, 0], projected_dataset_embeddings[:, 1], s=10, color='gray')\n",
    "# plt.scatter(projected_query_embedding[:, 0], projected_query_embedding[:, 1], s=50, marker='X', color='r')\n",
    "# plt.scatter(projected_retrieved_embeddings[:, 0], projected_retrieved_embeddings[:, 1], s=100, facecolors='none', edgecolors='g')\n",
    "\n",
    "# plt.gca().set_aspect('equal', 'datalim')\n",
    "# plt.title(f'{query}')\n",
    "# plt.show()\n",
    "# # plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# augmented_query_embedding = embedding_function([joint_query])\n",
    "# original_query_embedding = embedding_function([query])\n",
    "# retrieved_embeddings = relevant_text['embeddings'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# projected_original_query_embedding = project_embeddings(original_query_embedding, umap_transform)\n",
    "# projected_augmented_query_embedding = project_embeddings(augmented_query_embedding, umap_transform)\n",
    "# projected_retrieved_embeddings = project_embeddings(retrieved_embeddings, umap_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure()\n",
    "# plt.scatter(projected_dataset_embeddings[:, 0], projected_dataset_embeddings[:, 1], s=10, color='gray')\n",
    "# plt.scatter(projected_retrieved_embeddings[:, 0], projected_retrieved_embeddings[:, 1], s=100, facecolors='none', edgecolors='g')\n",
    "# plt.scatter(projected_original_query_embedding[:, 0], projected_original_query_embedding[:, 1], s=50, marker='X', color='r')\n",
    "# plt.scatter(projected_augmented_query_embedding[:, 0], projected_augmented_query_embedding[:, 1], s=50, marker='X', color='blue')\n",
    "\n",
    "# plt.gca().set_aspect('equal', 'datalim')\n",
    "# plt.title(f'{query}')\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
